import pandas as pd
import os
from pathlib import Path
from datetime import datetime
import re
from database import create_welding_table
from database import create_connection
from mysql.connector import Error
import tempfile
import csv
import math
import time
import logging

DATE_PATTERN = re.compile(r'^\d{4}-\d{2}-\d{2}$')

def resolve_welding_files(path_or_dir):
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥å¯»æ‰¾å®é™…çš„ç„Šæ¥æ•°æ®æ–‡ä»¶
    - æ”¯æŒä¼ å…¥å•ä¸ªæ–‡ä»¶
    - æ”¯æŒä¼ å…¥ç›®å½•ï¼Œè‡ªåŠ¨æšä¸¾ç›®å½•ä¸‹æ‰€æœ‰ WeldingDB_*.xlsx
    - æ”¯æŒé€šé…ç¬¦
    è¿”å›è·¯å¾„å­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŒ‰æ–‡ä»¶åæ’åº
    """
    if not path_or_dir:
        return []
    path_obj = Path(path_or_dir)
    if path_obj.is_file():
        return [str(path_obj)]
    if path_obj.is_dir():
        candidates = sorted(path_obj.glob("WeldingDB_*.xlsx"))
        return [str(p) for p in candidates]
    # å…è®¸ç›´æ¥ä¼  pattern
    matches = sorted(Path(path_obj.parent).glob(path_obj.name))
    return [str(p) for p in matches]


class WeldingDataImporter:
    # Excelåˆ—ååˆ°æ•°æ®åº“åˆ—åçš„æ˜ å°„
    EXCEL_COLUMNS = {
        'æ–½å·¥æ‰¿åŒ…å•†': 'ConstContractor',
        'ä»‹è´¨':'SystemCode',
        'å­ç³»ç»Ÿ':'SubSystemCode',
        'å›¾çº¸å·':'DrawingNumber',
        'ç‰ˆæœ¬å·':'RevNo',
        'é¡µç ':'PageNumber',
        'ç®¡çº¿å·':'PipelineNumber', 
        'æµç¨‹å›¾å·':'PIDDrawingNumber',
        'ç®¡é“ææ–™ç­‰çº§':'PipingMaterialClass',
        'å‹åŠ›ç­‰çº§':'PressureClass',
        'ä»‹è´¨çº§åˆ«':'MediumLevel',
        'ç®¡æ®µå·':'SpoolNo',
        'ç„Šç¼ç¼–å·': 'WeldJoint',
        'å®‰è£…/Fé¢„åˆ¶/S':'JointTypeFS',
        'è®¾è®¡æ¯”ä¾‹':'NDTDesignRatio',
        'æ¯ææè´¨1':'Material1',
        'æ¯ææè´¨2':'Material2',
        'å¤–å¾„1':'OuterDiameter1',
        'å¤–å¾„2':'OuterDiameter2',
        'åšåº¦1':'SCH1',
        'åšåº¦2':'SCH2',
        'ç„Šæ¥ç±»å‹':'WeldingType',
        'æ¥å¤´ç±»å‹(ä¿„æ ‡)':'WeldJointTypeRUS',
        'WPSç¼–å·':'WPSNumber',
        'ç„Šæ¥æ–¹æ³•(æ ¹å±‚)':'WeldMethodRoot',
        'ç„Šæ¥æ–¹æ³•(å¡«å……ã€ç›–é¢)':'WeldMethodCover',
        'ç„Šæ¥ç¯å¢ƒæ¸©åº¦â„ƒ':'WeldEnvironmentTemperature',
        'ç„Šå·¥å·æ ¹å±‚':'WelderRoot',
        'ç„Šå·¥å·å¡«å……ã€ç›–é¢':'WelderFill',
        'æ˜¯å¦çƒ­å¤„ç†':'IsHeatTreatment',
        'çƒ­å¤„ç†æ—¥æœŸ':'HeatTreatmentDate',
        'çƒ­å¤„ç†æŠ¥å‘Šå·':'HeatTreatmentReportNumber',
        'çƒ­å¤„ç†å·¥':'HeatTreatmentWorker',
        'è¯•å‹åŒ…å·': 'TestPackageID',
        'ç„Šæ¥æ—¥æœŸ': 'WeldDate',
        'å°ºå¯¸': 'Size',
        'VTæŠ¥å‘Šå·':'VTReportNumber',
        'VTæŠ¥å‘Šæ—¥æœŸ':'VTReportDate',
        'VTæ£€æµ‹ç»“æœ': 'VTResult',
        'RTæŠ¥å‘Šå·':'RTReportNumber',
        'RTæŠ¥å‘Šæ—¥æœŸ':'RTReportDate',
        'RTæ£€æµ‹ç»“æœ': 'RTResult',
        'PTæŠ¥å‘Šå·':'PTReportNumber',
        'PTæŠ¥å‘Šæ—¥æœŸ':'PTReportDate',
        'PTæ£€æµ‹ç»“æœ': 'PTResult',
        'UTæŠ¥å‘Šå·':'UTReportNumber',
        'UTæŠ¥å‘Šæ—¥æœŸ':'UTReportDate',
        'UTæ£€æµ‹ç»“æœ': 'UTResult',
        'MTæŠ¥å‘Šå·':'MTReportNumber',
        'MTæŠ¥å‘Šæ—¥æœŸ':'MTReportDate',
        'MTæ£€æµ‹ç»“æœ': 'MTResult',
        'PMIæŠ¥å‘Šå·':'PMIReportNumber',
        'PMIæŠ¥å‘Šæ—¥æœŸ':'PMIReportDate',
        'PMIæ£€æµ‹ç»“æœ': 'PMIResult',
        'FTæŠ¥å‘Šå·':'FTReportNumber',
        'FTæŠ¥å‘Šæ—¥æœŸ':'FTReportDate',
        'FTæ£€æµ‹ç»“æœ': 'FTResult',
        'HTæŠ¥å‘Šå·':'HTReportNumber',
        'HTæŠ¥å‘Šæ—¥æœŸ':'HTReportDate',
        'HTæ£€æµ‹ç»“æœ':'HTResult',
        'PWHTæŠ¥å‘Šå·':'PWHTReportNumber',
        'PWHTæŠ¥å‘Šæ—¥æœŸ':'PWHTReportDate',
        'PWHTæ£€æµ‹ç»“æœ':'PWHTResult',
        'ç„Šå£çŠ¶æ€':'JointStatus'
    }
    DATE_SOURCE_COLUMNS = [
        'ç„Šæ¥æ—¥æœŸ',
        'çƒ­å¤„ç†æ—¥æœŸ',
        'VTæŠ¥å‘Šæ—¥æœŸ',
        'RTæŠ¥å‘Šæ—¥æœŸ',
        'PTæŠ¥å‘Šæ—¥æœŸ',
        'UTæŠ¥å‘Šæ—¥æœŸ',
        'MTæŠ¥å‘Šæ—¥æœŸ',
        'PMIæŠ¥å‘Šæ—¥æœŸ',
        'FTæŠ¥å‘Šæ—¥æœŸ',
        'HTæŠ¥å‘Šæ—¥æœŸ',
        'PWHTæŠ¥å‘Šæ—¥æœŸ'
    ]
    
    CHUNK_SIZE = 50000  # å¯¼å…¥åˆ†ç‰‡å¤§å°

    def __init__(self, excel_path, verbose=False):
        self.excel_files = resolve_welding_files(excel_path)
        if not self.excel_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç„Šæ¥æ•°æ®æ–‡ä»¶: {excel_path}")
        self.excel_path = self.excel_files[0]
        self.df = None
        self.verbose = verbose
        self.invalid_date_records = []
        self.load_data()
    
    def load_data(self):
        """åŠ è½½Excelæ•°æ®ï¼ˆå‚è€ƒåŸåŠ è½½é€»è¾‘ï¼‰"""
        try:
            data_frames = []
            total_rows = 0
            self.invalid_date_records = []
            for idx, excel_file in enumerate(self.excel_files, start=1):
                if not os.path.exists(excel_file):
                    print(f"WARNING: File not found: {excel_file}")
                    continue
                raw_df = pd.read_excel(excel_file, header=1, dtype=str)
                raw_df = raw_df.where(raw_df.notna(), None)
                normalized_columns = []
                for col in raw_df.columns:
                    if isinstance(col, str):
                        first_line = col.splitlines()[0].strip()
                        normalized_columns.append(first_line)
                    else:
                        normalized_columns.append(col)
                raw_df.columns = normalized_columns
                for date_col in self.DATE_SOURCE_COLUMNS:
                    self._collect_invalid_dates(raw_df, date_col, os.path.basename(excel_file))
                data_frames.append(raw_df)
                total_rows += len(raw_df)
                print(f"SUCCESS: Loaded {len(raw_df)} rows from welding data ({os.path.basename(excel_file)})")
            if data_frames:
                self.df = pd.concat(data_frames, ignore_index=True)
                print(f"ğŸ“ˆ åˆè®¡åŠ è½½ç„Šå£æ•°æ® {total_rows} è¡Œï¼Œæ¥æºæ–‡ä»¶ {len(data_frames)} ä¸ª")
                if self.verbose:
                    print("Excel column mapping:")
                    for excel_col, db_col in self.EXCEL_COLUMNS.items():
                        if excel_col in self.df.columns:
                            print(f"  '{excel_col}' -> '{db_col}'")
                        else:
                            print(f"  WARNING: Column not found: '{excel_col}'")
                self._write_invalid_date_log()
            else:
                print("ERROR: æœªæˆåŠŸè¯»å–ä»»ä½•ç„Šæ¥æ•°æ®æ–‡ä»¶")
                self.df = pd.DataFrame()
        except Exception as e:
            print(f"ERROR: Failed to load Excel: {e}")
            self.df = pd.DataFrame()

    def _collect_invalid_dates(self, df, column_name, source_file):
        if column_name not in df.columns:
            return
        series = df[column_name]
        for idx, val in series.items():
            if val is None or (isinstance(val, float) and math.isnan(val)):
                continue
            text = str(val).strip()
            if not text or text.lower() in {'nan', 'nat', 'none', 'null'}:
                continue
            if DATE_PATTERN.match(text):
                continue
            self.invalid_date_records.append({
                'SourceFile': source_file,
                'Column': column_name,
                'ExcelRow': idx + 2,  # header=1 æ•°æ®ä»ç¬¬2è¡Œå¼€å§‹
                'RawValue': text
            })

    def _write_invalid_date_log(self):
        if not self.invalid_date_records:
            return
        out_path = Path(self.excel_path).parent / 'invalid_weld_dates.csv'
        try:
            pd.DataFrame(self.invalid_date_records).to_csv(out_path, index=False, encoding='utf-8-sig')
            print(f"[WARN] æ£€æµ‹åˆ° {len(self.invalid_date_records)} ä¸ªéæ ‡å‡†æ—¥æœŸå€¼ï¼Œè¯¦è§ {out_path}")
        except Exception as log_error:
            print(f"[WARN] å†™å…¥ invalid_weld_dates.csv å¤±è´¥: {log_error}")
    
    def _retry_connection(self, max_retries=5, initial_delay=2, max_delay=60):
        """
        é‡è¯•è·å–æ•°æ®åº“è¿æ¥
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼ŒæŒ‡æ•°é€€é¿ä¸Šé™ï¼‰
        """
        delay = initial_delay
        for attempt in range(max_retries):
            try:
                connection = create_connection()
                if connection and connection.is_connected():
                    if attempt > 0:
                        print(f"âœ… è¿æ¥æˆåŠŸï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰")
                    return connection
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¿æ¥å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}ï¼Œ{delay}ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay = min(delay * 2, max_delay)  # æŒ‡æ•°é€€é¿
                else:
                    print(f"âŒ è¿æ¥å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}")
        return None
    
    def _retry_execute(self, connection, cursor, sql, max_retries=5, initial_delay=2, max_delay=60):
        """
        é‡è¯•æ‰§è¡ŒSQLè¯­å¥
        """
        delay = initial_delay
        for attempt in range(max_retries):
            try:
                # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
                if not connection.is_connected():
                    print(f"âš ï¸  è¿æ¥å·²æ–­å¼€ï¼Œå°è¯•é‡æ–°è¿æ¥...")
                    connection.close()
                    connection = self._retry_connection()
                    if not connection:
                        raise Error("æ— æ³•é‡æ–°å»ºç«‹è¿æ¥")
                    cursor = connection.cursor()
                
                cursor.execute(sql)
                return True, connection, cursor
            except Error as e:
                error_msg = str(e)
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯
                is_retryable = any(keyword in error_msg.lower() for keyword in [
                    'lost connection', 'connection', 'timeout', 'gone away', 
                    'server has gone away', 'broken pipe', 'network'
                ])
                
                if is_retryable and attempt < max_retries - 1:
                    print(f"âš ï¸  SQLæ‰§è¡Œå¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}ï¼Œ{delay}ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay = min(delay * 2, max_delay)  # æŒ‡æ•°é€€é¿
                    
                    # å°è¯•é‡æ–°è¿æ¥
                    try:
                        connection.close()
                    except:
                        pass
                    connection = self._retry_connection()
                    if connection:
                        cursor = connection.cursor()
                else:
                    raise
        return False, connection, cursor
    
    def import_to_database(self):
        """å°†æ•°æ®å¯¼å…¥æ•°æ®åº“ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        if self.df is None or self.df.empty:
            print("ERROR: No data to import")
            return False
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–è¿æ¥
        connection = self._retry_connection()
        if not connection:
            print("ERROR: æ— æ³•å»ºç«‹æ•°æ®åº“è¿æ¥ï¼Œå·²é‡è¯•å¤šæ¬¡")
            return False
        
        cursor = None
        try:
            cursor = connection.cursor()
            checks_disabled = False
            try:
                cursor.execute("SET FOREIGN_KEY_CHECKS = 0")
                cursor.execute("SET UNIQUE_CHECKS = 0")
                checks_disabled = True
            except Exception:
                pass
            # æ¸…ç©ºè¡¨ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚å†³å®šæ˜¯å¦ä¿ç•™å†å²æ•°æ®ï¼‰
            cursor.execute("TRUNCATE TABLE WeldingList")
            # ç›®æ ‡åˆ—é¡ºåºï¼ˆä¸è¡¨ç»“æ„ä¸€è‡´ä¸”ä»…åŒ…å«å®é™…éœ€è¦åˆ—ï¼‰
            target_columns = [
                # åŸºç¡€ä¿¡æ¯
                'WeldID', 'ConstContractor', 'SystemCode', 'SubSystemCode', 'WeldJoint', 'JointTypeFS',
                'DrawingNumber', 'PageNumber', 'RevNo', 'PID', 'PIDDrawingNumber',
                # ç®¡é“ææ–™ä¿¡æ¯
                'PipingMaterialClass', 'PressureClass', 'MediumLevel', 'SpoolNo', 'NDTDesignRatio',
                'Material1', 'Material2', 'OuterDiameter1', 'OuterDiameter2', 'SCH1', 'SCH2',
                # ç„Šæ¥ä¿¡æ¯
                'WeldingType', 'WeldJointTypeRUS', 'PipelineNumber', 'TestPackageID', 'WeldDate', 'Size',
                # ç„Šå·¥å’ŒWPS
                'WelderRoot', 'WelderFill', 'WPSNumber', 'WeldMethodRoot', 'WeldMethodCover', 'WeldEnvironmentTemperature',
                # çƒ­å¤„ç†
                'IsHeatTreatment', 'HeatTreatmentDate', 'HeatTreatmentReportNumber', 'HeatTreatmentWorker',
                # VTæ£€æµ‹
                'VTReportNumber', 'VTReportDate', 'VTResult',
                # RTæ£€æµ‹
                'RTReportNumber', 'RTReportDate', 'RTResult',
                # UTæ£€æµ‹
                'UTReportNumber', 'UTReportDate', 'UTResult',
                # PTæ£€æµ‹
                'PTReportNumber', 'PTReportDate', 'PTResult',
                # HTæ£€æµ‹
                'HTReportNumber', 'HTReportDate', 'HTResult',
                # PWHTæ£€æµ‹
                'PWHTReportNumber', 'PWHTReportDate', 'PWHTResult',
                # MTæ£€æµ‹
                'MTReportNumber', 'MTReportDate', 'MTResult',
                # PMIæ£€æµ‹
                'PMIReportNumber', 'PMIReportDate', 'PMIResult',
                # FTæ£€æµ‹
                'FTReportNumber', 'FTReportDate', 'FTResult',
                # çŠ¶æ€
                'Status', 'JointStatus'
            ]

            # ä»åŸå§‹DataFrameç”Ÿæˆæœ‰æ•ˆè®°å½•çš„æ•°æ®æ¡†
            df = self.df.copy()

            # ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ–‡æœ¬å­—æ®µï¼ˆä½¿ç”¨excel_columnsæ˜ å°„ï¼‰
            def extract_text_field(df, excel_col_name, default=''):
                """ä»Excelåˆ—æå–æ–‡æœ¬å­—æ®µ"""
                if excel_col_name in df.columns:
                    return df[excel_col_name].astype(str).str.strip()
                else:
                    return default
            
            # æå–æ‰€æœ‰å­—æ®µ
            for excel_col, db_col in self.EXCEL_COLUMNS.items():
                if excel_col not in ['ç„Šæ¥æ—¥æœŸ', 'çƒ­å¤„ç†æ—¥æœŸ'] and 'date' not in excel_col.lower():  # æ—¥æœŸå­—æ®µå•ç‹¬å¤„ç†
                    df[db_col] = extract_text_field(df, excel_col)
            
            # WelderFillå·²åœ¨ç»Ÿä¸€æå–ä¸­å¤„ç†
            
            # å¦‚æœæŸäº›å­—æ®µä¸åœ¨æ˜ å°„ä¸­ï¼Œæ‰‹åŠ¨æ·»åŠ 
            if 'PID' not in df.columns:
                df['PID'] = ''
            # WeldID = DrawingNumber-PipelineNumber-WeldJointï¼›è‹¥ä¸‰è€…çš†ç©ºï¼Œåˆ™ç”Ÿæˆ AUTO-<index>
            def compose_weld_id(r):
                parts = [r['DrawingNumber'], r['PageNumber'], r['PipelineNumber'], r['WeldJoint']]
                parts = [p for p in parts if p]
                return '-'.join(parts) if parts else 'AUTO-' + str(r.name)
            df['WeldID'] = df.apply(compose_weld_id, axis=1)

            # é¢„æµ‹æœ‰æ•ˆè¡Œæ•°
            predicted_rows = len(df)

            # åˆ—æ˜ å°„ä¸è½¬æ¢
            df['TestPackageID'] = df['è¯•å‹åŒ…å·'].astype(str).str.strip() if 'è¯•å‹åŒ…å·' in df.columns else ''
            df['SystemCode'] = df['ä»‹è´¨'].astype(str).str.strip() if 'ä»‹è´¨' in df.columns else 'UNDEFINED'
            df['SystemCode'] = df['SystemCode'].apply(
                lambda x: x if x and str(x).strip().lower() not in {'nan', 'none', 'null'} else 'UNDEFINED'
            )
            df['SubSystemCode'] = df['å­ç³»ç»Ÿ'].astype(str).str.strip() if 'å­ç³»ç»Ÿ' in df.columns else ''

            def ensure_subsystem(row):
                subsystem = row.get('SubSystemCode')
                if subsystem:
                    return subsystem
                if row.get('SystemCode'):
                    return f"{row['SystemCode']}_UNDEFINED"
                if row.get('TestPackageID'):
                    return f"{row['TestPackageID']}_UNDEFINED"
                return "UNDEFINED"

            df['SubSystemCode'] = df.apply(ensure_subsystem, axis=1)

            # å…ˆæ ¹æ®Excelæ•°æ®å¡«å……ç³»ç»Ÿ/å­ç³»ç»Ÿ/è¯•å‹åŒ…ä¸»æ•°æ®ï¼ˆå¦‚ä¸å­˜åœ¨åˆ™æ’å…¥ï¼Œå ä½æè¿°åç»­ç»´æŠ¤ï¼‰
            try:
                unique_systems = sorted(set([c for c in df['SystemCode'].tolist() if c]))
                if unique_systems:
                    sys_values = [(sc, sc, None, 'Process', 0, '', 'admin', 'admin') for sc in unique_systems]
                    cursor.executemany(
                        """
                        INSERT IGNORE INTO SystemList
                        (SystemCode, SystemDescriptionENG, SystemDescriptionRUS, ProcessOrNonProcess, Priority, Remarks, created_by, last_updated_by)
                        VALUES (%s,%s,%s,%s,%s,%s,%s,%s)
                        """,
                        sys_values
                    )
                    connection.commit()  # ç«‹å³æäº¤
                    
                unique_subs = set()
                for _, r in df[['SubSystemCode','SystemCode']].drop_duplicates().iterrows():
                    scode = str(r['SystemCode']).strip()
                    sub = str(r['SubSystemCode']).strip()
                    if scode and sub:
                        unique_subs.add((sub, scode))
                if unique_subs:
                    sub_values = [(sub, sysc, sub, None, 'Process', 0, '', 'admin', 'admin') for (sub, sysc) in unique_subs]
                    cursor.executemany(
                        """
                        INSERT IGNORE INTO SubsystemList
                        (SubSystemCode, SystemCode, SubSystemDescriptionENG, SubSystemDescriptionRUS, ProcessOrNonProcess, Priority, Remarks, created_by, last_updated_by)
                        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)
                        """,
                        sub_values
                    )
                    connection.commit()  # ç«‹å³æäº¤
                    
                unique_tps = set()
                for _, r in df[['TestPackageID','SystemCode','SubSystemCode']].drop_duplicates().iterrows():
                    tpid = str(r['TestPackageID']).strip()
                    scode = str(r['SystemCode']).strip()
                    sub = str(r['SubSystemCode']).strip()
                    if tpid:
                        unique_tps.add((tpid, scode if scode else None, sub if sub else None))
                if unique_tps:
                    tp_values = [(tpid, scode, sub, tpid, None, None, 'Pending', None, None, '', 'admin', 'admin') for (tpid, scode, sub) in unique_tps]
                    cursor.executemany(
                        """
                        INSERT IGNORE INTO HydroTestPackageList
                        (TestPackageID, SystemCode, SubSystemCode, Description, PlannedDate, ActualDate, Status, Pressure, TestDuration, Remarks, created_by, last_updated_by)
                        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
                        """,
                        tp_values
                    )
                    connection.commit()  # ç«‹å³æäº¤
                    
            except Exception as seed_e:
                if self.verbose:
                    print(f"WARNING: Failed to seed master data: {seed_e}")

            # æ—¥æœŸæ ‡å‡†åŒ–ä¸ºYYYY-MM-DD
            def to_date_str(v):
                if v is None or (isinstance(v, float) and math.isnan(v)):
                    return None
                text = str(v).strip()
                if text == '' or text.lower() in {'nan', 'nat', 'none', 'null'}:
                    return None
                if DATE_PATTERN.match(text):
                    return text
                dt = pd.to_datetime(text, errors='coerce')
                if pd.isna(dt):
                    return None
                formatted = dt.strftime('%Y-%m-%d')
                return formatted if DATE_PATTERN.match(formatted) else None
            
            # å¤„ç†æ‰€æœ‰æ—¥æœŸå­—æ®µ
            date_field_mapping = {
                'ç„Šæ¥æ—¥æœŸ': 'WeldDate',
                'çƒ­å¤„ç†æ—¥æœŸ': 'HeatTreatmentDate',
                'VTæŠ¥å‘Šæ—¥æœŸ': 'VTReportDate',
                'RTæŠ¥å‘Šæ—¥æœŸ': 'RTReportDate',
                'PTæŠ¥å‘Šæ—¥æœŸ': 'PTReportDate',
                'UTæŠ¥å‘Šæ—¥æœŸ': 'UTReportDate',
                'MTæŠ¥å‘Šæ—¥æœŸ': 'MTReportDate',
                'PMIæŠ¥å‘Šæ—¥æœŸ': 'PMIReportDate',
                'FTæŠ¥å‘Šæ—¥æœŸ': 'FTReportDate',
                'HTæŠ¥å‘Šæ—¥æœŸ': 'HTReportDate',
                'PWHTæŠ¥å‘Šæ—¥æœŸ': 'PWHTReportDate'
            }
            
            for excel_col, db_col in date_field_mapping.items():
                if excel_col in df.columns:
                    df[db_col] = df[excel_col].apply(to_date_str)
                else:
                    df[db_col] = None
            date_cols = list(date_field_mapping.values())

            def normalize_date_series(series):
                if series is None:
                    return None
                def _normalize(val):
                    if val is None or (isinstance(val, float) and math.isnan(val)):
                        return None
                    text = str(val).strip()
                    if text == '' or text.lower() in {'nan', 'nat', 'none', 'null'}:
                        return None
                    if DATE_PATTERN.match(text):
                        return text
                    dt = pd.to_datetime(text, errors='coerce')
                    if pd.isna(dt):
                        return None
                    formatted = dt.strftime('%Y-%m-%d')
                    return formatted if DATE_PATTERN.match(formatted) else None
                return series.apply(_normalize)

            for db_col in date_cols:
                df[db_col] = normalize_date_series(df.get(db_col))

            # å°ºå¯¸ä¸ºæ•°å€¼æˆ–NULL
            def to_size(v):
                try:
                    return float(v)
                except Exception:
                    return None
            df['Size'] = df['å°ºå¯¸'].apply(lambda v: to_size(v) if pd.notna(v) else None)

            # æ£€æµ‹ç»“æœå­—æ®µ
            def to_str_series(column_name: str):
                if column_name in df.columns:
                    s = df[column_name]
                    s = s.where(pd.notna(s), '')  # NaN -> ''
                    return s.astype(str).str.strip()
                else:
                    return pd.Series([''] * len(df))

            for cn, en in [
                ('VTæ£€æµ‹ç»“æœ', 'VTResult'),
                ('RTæ£€æµ‹ç»“æœ', 'RTResult'),
                ('UTæ£€æµ‹ç»“æœ', 'UTResult'),
                ('PTæ£€æµ‹ç»“æœ', 'PTResult'),
                ('HTæ£€æµ‹ç»“æœ', 'HTResult'),
                ('PWHTæ£€æµ‹ç»“æœ', 'PWHTResult'),
                ('MTæ£€æµ‹ç»“æœ', 'MTResult'),
                ('PMIæ£€æµ‹ç»“æœ', 'PMIResult'),
                ('FTæ£€æµ‹ç»“æœ', 'FTResult'),
            ]:
                df[en] = to_str_series(cn)

            # ç„Šå·¥/WPS å­—æ®µå·²åœ¨ç»Ÿä¸€æå–ä¸­å¤„ç†

            # é¢å¤–æ´¾ç”Ÿåˆ—ï¼šæµ‹è¯•ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼Œä»…ç”¨äºç»Ÿè®¡/æ ¡éªŒï¼‰
            test_source_cols = ['VTæ£€æµ‹ç»“æœ', 'RTæ£€æµ‹ç»“æœ', 'UTæ£€æµ‹ç»“æœ', 'PTæ£€æµ‹ç»“æœ', 'HTæ£€æµ‹ç»“æœ', 'PWHTæ£€æµ‹ç»“æœ', 'MTæ£€æµ‹ç»“æœ', 'PMIæ£€æµ‹ç»“æœ', 'FTæ£€æµ‹ç»“æœ']
            def derive_test_status(row):
                for c in test_source_cols:
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip() != '':
                        return 'å·²å®Œæˆ'
                return 'æœªå®Œæˆ'
            df['æµ‹è¯•'] = df.apply(derive_test_status, axis=1)

            # çŠ¶æ€ï¼šæ‰€æœ‰æµ‹è¯•ç»“æœå‡ä¸º"åˆæ ¼"æ—¶æ ‡è®°ä¸ºå·²å®Œæˆï¼Œå¦åˆ™æœªå®Œæˆ
            def evaluate_status(row):
                cols = ['VTResult', 'RTResult', 'UTResult', 'PTResult', 'HTResult', 'PWHTResult', 'MTResult', 'PMIResult', 'FTResult']
                values = [str(row.get(c) or '').strip() for c in cols]
                non_empty = [v for v in values if v != '']
                # åªè¦æœ‰ä¸€ä¸ªéç©ºä¸”ä¸ä¸º"åˆæ ¼"ï¼Œå³æœªå®Œæˆ
                if any(v != 'åˆæ ¼' for v in non_empty):
                    return 'æœªå®Œæˆ'
                # æ²¡æœ‰ä¸åˆæ ¼ï¼Œä¸”è‡³å°‘ä¸€ä¸ªä¸º"åˆæ ¼"ï¼Œåˆ™å·²å®Œæˆ
                if any(v == 'åˆæ ¼' for v in non_empty):
                    return 'å·²å®Œæˆ'
                # å…¨éƒ¨ä¸ºç©º
                return 'æœªå®Œæˆ'
            df['Status'] = df.apply(evaluate_status, axis=1)

            # ä»…ä¿ç•™ç›®æ ‡åˆ—é¡ºåº
            export_df = df[target_columns]

            # å°†æ‰€æœ‰å­—ç¬¦ä¸²åˆ—çš„ç©ºå­—ç¬¦ä¸²æ ‡å‡†åŒ–ä¸º NULLï¼ˆä»¥å†™å‡ºä¸º \\Nï¼‰
            for col in export_df.columns:
                if export_df[col].dtype == 'object':  # å­—ç¬¦ä¸²ç±»å‹åˆ—
                    export_df.loc[:, col] = export_df[col].apply(
                        lambda x: None if (isinstance(x, str) and x.strip() in ['', 'nan', 'NaN', 'None']) else x
                    )
            date_regex = DATE_PATTERN.pattern
            for date_col in date_cols:
                if date_col in export_df.columns:
                    export_df.loc[:, date_col] = normalize_date_series(export_df[date_col])
                    series = export_df[date_col]
                    value_str = series.astype(str)
                    invalid_mask = series.notna() & ~value_str.str.fullmatch(date_regex)
                    if invalid_mask.any():
                        sample_values = value_str[invalid_mask].head(3).tolist()
                        sample_ids = export_df.loc[invalid_mask, 'WeldID'].head(3).tolist() if 'WeldID' in export_df.columns else []
                        print(f"[WARN] {date_col} æ£€æµ‹åˆ° {invalid_mask.sum()} ä¸ªéæ³•æ—¥æœŸå€¼, ç¤ºä¾‹: {sample_values}, WeldID: {sample_ids}")
                        for sample_value, sample_weld in zip(sample_values, sample_ids):
                            self.invalid_date_records.append({
                                'SourceFile': 'merged',
                                'Column': date_col,
                                'ExcelRow': None,
                                'WeldID': sample_weld,
                                'RawValue': sample_value
                            })
                        export_df.loc[invalid_mask, date_col] = None

            # è¯Šæ–­ï¼šä»…åœ¨verboseæ—¶æ‰“å°å”¯ä¸€å€¼/é‡å¤æƒ…å†µ
            if self.verbose:
                try:
                    unique_weld_ids = export_df['WeldID'].nunique(dropna=False)
                    dup_rows = len(export_df) - unique_weld_ids
                    print(f"ğŸ“ˆ ç„Šç¼ç¼–å·å”¯ä¸€å€¼ {unique_weld_ids}ï¼Œé‡å¤è¡Œ {dup_rows}")
                    if dup_rows > 0:
                        dup_sample = export_df['WeldID'].value_counts().head(5)
                        print("ğŸ” é‡å¤æœ€å¤šçš„å‰5ä¸ªç„Šç¼ç¼–å·ï¼š")
                        for wid, cnt in dup_sample.items():
                            if cnt > 1:
                                print(f"  {wid}: {cnt} æ¬¡")
                except Exception:
                    pass

            total_loaded = 0
            if len(export_df) == 0:
                chunk_iterator = [export_df]
            else:
                chunk_iterator = (
                    export_df.iloc[start:start + self.CHUNK_SIZE]
                    for start in range(0, len(export_df), self.CHUNK_SIZE)
                )

            # è®°å½•æœ€è¿‘ä¸€æ¬¡å†™å‡ºçš„ä¸´æ—¶CSVè·¯å¾„ï¼Œä¾¿äºå‡ºé”™æ—¶æ’æŸ¥
            last_temp_csv_path = None

            try:
                cursor.execute("SET SESSION local_infile = 1")
            except Exception:
                pass

            # ä¸ºäº†åœ¨ MySQL ç«¯æ›´å®‰å…¨åœ°å¤„ç†æ—¥æœŸï¼Œå°† WeldDate å…ˆè¯»å…¥ç”¨æˆ·å˜é‡ï¼Œå†ç”¨ STR_TO_DATE è½¬æ¢ã€‚
            # è¿™æ ·å³ä½¿æŸè¡Œåˆ—é”™ä½å¯¼è‡´ '1.0' ä¹‹ç±»çš„å€¼è¿›å…¥ï¼Œä¹Ÿä¼šè¢«è½¬æ¢ä¸º NULLï¼Œè€Œä¸ä¼šæŠ› 1292 é”™è¯¯ã€‚
            load_columns = []
            for col in target_columns:
                if col == 'WeldDate':
                    load_columns.append('@tmp_WeldDate')
                else:
                    load_columns.append(col)
            columns_str = ', '.join(load_columns)

            for chunk in chunk_iterator:
                if chunk.empty:
                    continue

                # é˜²å¾¡æ€§æ¸…æ´—ï¼š
                # - å»æ‰æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µä¸­çš„æ¢è¡Œç¬¦ï¼Œé˜²æ­¢ CSV è¡Œç»“æ„è¢«æ‹†æˆå¤šè¡Œå¯¼è‡´åˆ—é”™ä½
                # - å»æ‰æ™®é€šå­—ç¬¦ä¸²ä¸­çš„åæ–œæ ï¼Œé¿å…ä¸ ESCAPED BY '\\' ç»„åˆæˆ \" ä¹‹ç±»å¯¼è‡´ MySQL è¯¯è§£æ
                for col in chunk.columns:
                    if chunk[col].dtype == 'object':
                        chunk.loc[:, col] = chunk[col].apply(
                            lambda v: (
                                v.replace('\r', ' ')
                                 .replace('\n', ' ')
                                 .replace('\\', ' ')
                            ) if isinstance(v, str) else v
                        )

                # äºŒæ¬¡æ ¡éªŒæ—¥æœŸåˆ—ï¼Œç¡®ä¿ chunk ä¸­ä¸ä¼šæ®‹ç•™éæ³•å€¼
                for date_col in date_cols:
                    if date_col in chunk.columns:
                        chunk_series = chunk[date_col]
                        chunk_str = chunk_series.astype(str)
                        invalid_mask = chunk_series.notna() & ~chunk_str.str.fullmatch(date_regex)
                        if invalid_mask.any():
                            sample_vals = chunk_str[invalid_mask].head(3).tolist()
                            sample_ids = chunk.loc[invalid_mask, 'WeldID'].head(3).tolist() if 'WeldID' in chunk.columns else []
                            print(f"[WARN] chunk ä¸­ {date_col} å‘ç° {invalid_mask.sum()} ä¸ªéæ³•å€¼, ç¤ºä¾‹å€¼: {sample_vals}, WeldID: {sample_ids}")
                            for sample_value, sample_weld in zip(sample_vals, sample_ids):
                                self.invalid_date_records.append({
                                    'SourceFile': 'chunk',
                                    'Column': date_col,
                                    'ExcelRow': None,
                                    'WeldID': sample_weld,
                                    'RawValue': sample_value
                                })
                            chunk.loc[invalid_mask, date_col] = None

                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, newline='', encoding='utf-8') as tmp:
                    temp_csv_path = tmp.name
                    # ä½¿ç”¨ QUOTE_ALL å¹¶åœ¨æ¸…æ´—æ¢è¡Œåå†™å‡ºï¼Œé™ä½åˆ—é”™ä½é£é™©
                    chunk.to_csv(
                        tmp,
                        index=False,
                        na_rep='\\N',
                        lineterminator='\r\n',
                        quoting=csv.QUOTE_ALL
                    )

                last_temp_csv_path = temp_csv_path
                print(f"[DEBUG] Temp CSV path: {temp_csv_path}, rows in chunk: {len(chunk)}")

                escaped_path = temp_csv_path.replace('\\', r'\\')
                load_sql = (
                    f"LOAD DATA LOCAL INFILE '{escaped_path}' REPLACE INTO TABLE WeldingList "
                    "CHARACTER SET utf8mb4 "
                    "FIELDS TERMINATED BY ',' ENCLOSED BY '\"' ESCAPED BY '\\\\' "
                    "LINES TERMINATED BY '\r\n' "
                    "IGNORE 1 LINES "
                    f"({columns_str}) "
                    "SET WeldDate = STR_TO_DATE(NULLIF(@tmp_WeldDate, ''), '%Y-%m-%d')"
                )
                
                # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡ŒLOAD DATA
                chunk_retry_delay = 2
                chunk_max_retries = 5
                chunk_loaded = False
                
                for chunk_attempt in range(chunk_max_retries):
                    try:
                        # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
                        if not connection.is_connected():
                            print(f"âš ï¸  è¿æ¥å·²æ–­å¼€ï¼Œå°è¯•é‡æ–°è¿æ¥ï¼ˆchunk {total_loaded // self.CHUNK_SIZE + 1}ï¼‰...")
                            try:
                                cursor.close()
                                connection.close()
                            except:
                                pass
                            connection = self._retry_connection()
                            if not connection:
                                raise Error("æ— æ³•é‡æ–°å»ºç«‹è¿æ¥")
                            cursor = connection.cursor()
                            # é‡æ–°è®¾ç½®ä¼šè¯å‚æ•°
                            try:
                                cursor.execute("SET SESSION local_infile = 1")
                                if checks_disabled:
                                    cursor.execute("SET FOREIGN_KEY_CHECKS = 0")
                                    cursor.execute("SET UNIQUE_CHECKS = 0")
                            except:
                                pass
                        
                        cursor.execute(load_sql)
                        connection.commit()
                        total_loaded += len(chunk)
                        chunk_loaded = True
                        if chunk_attempt > 0:
                            print(f"âœ… Chunk {total_loaded // self.CHUNK_SIZE} å¯¼å…¥æˆåŠŸï¼ˆç¬¬ {chunk_attempt + 1} æ¬¡å°è¯•ï¼‰")
                        break
                    except Error as e:
                        error_msg = str(e)
                        is_retryable = any(keyword in error_msg.lower() for keyword in [
                            'lost connection', 'connection', 'timeout', 'gone away',
                            'server has gone away', 'broken pipe', 'network'
                        ])
                        
                        if is_retryable and chunk_attempt < chunk_max_retries - 1:
                            print(f"âš ï¸  Chunkå¯¼å…¥å¤±è´¥ï¼ˆå°è¯• {chunk_attempt + 1}/{chunk_max_retries}ï¼‰: {e}ï¼Œ{chunk_retry_delay}ç§’åé‡è¯•...")
                            time.sleep(chunk_retry_delay)
                            chunk_retry_delay = min(chunk_retry_delay * 2, 60)  # æŒ‡æ•°é€€é¿
                        else:
                            print(f"âŒ Chunkå¯¼å…¥å¤±è´¥ï¼ˆå·²é‡è¯• {chunk_max_retries} æ¬¡ï¼‰: {e}")
                            raise
                
                if not chunk_loaded:
                    raise Error(f"Chunkå¯¼å…¥å¤±è´¥ï¼Œå·²é‡è¯• {chunk_max_retries} æ¬¡")
                #try:
                #    os.remove(temp_csv_path)
                #except Exception:
                #    pass

            # ä½¿ç”¨å®é™…è¡¨è®¡æ•°ï¼Œè€Œä¸æ˜¯å—å½±å“è¡Œæ•°ï¼ˆREPLACE ä¼š2å€è®¡æ•°ï¼‰
            try:
                cursor.execute("SELECT COUNT(*) FROM WeldingList")
                table_count = cursor.fetchone()[0]
            except Exception:
                table_count = None
            print(f"SUCCESS: Table now contains {table_count} rows (recent batch total {total_loaded})")
            # è¯Šæ–­ï¼šæ˜¾ç¤ºå‰10æ¡è­¦å‘Šï¼ˆè‹¥æœ‰ï¼‰
            if self.verbose:
                try:
                    cursor.execute("SHOW COUNT(*) WARNINGS")
                    warn_count = cursor.fetchone()
                    if warn_count:
                        print(f"WARNING count: {warn_count[0]}")
                    cursor.execute("SHOW WARNINGS LIMIT 10")
                    warnings = cursor.fetchall()
                    if warnings:
                        print("WARNINGS (first 10):")
                        for w in warnings:
                            print(str(w))
                except Exception:
                    pass

            # æ ¡éªŒ
            if self.verbose and (table_count is not None and table_count != predicted_rows):
                print("WARNING: Imported row count does not match expected, please check source data or CSV format.")

            # æµ‹è¯•çŠ¶æ€æ±‡æ€»ä»…åœ¨ verbose æ¨¡å¼æ‰“å°
            if self.verbose:
                try:
                    completed = (df['æµ‹è¯•'] == 'å·²å®Œæˆ').sum()
                    pending = (df['æµ‹è¯•'] == 'æœªå®Œæˆ').sum()
                    print(f"ğŸ“Š æµ‹è¯•çŠ¶æ€ï¼šå·²å®Œæˆ {completed}ï¼Œæœªå®Œæˆ {pending}")
                except Exception:
                    pass

            return True
        except Error as e:
            print(f"ERROR: Database operation failed: {e}")
            connection.rollback()
            return False
        finally:
            if connection and cursor:
                try:
                    if checks_disabled:
                        cursor.execute("SET FOREIGN_KEY_CHECKS = 1")
                        cursor.execute("SET UNIQUE_CHECKS = 1")
                except Exception:
                    pass
            if connection:
                connection.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç›®æ ‡Excelè·¯å¾„/ç›®å½•ï¼ˆç›®å½•æ—¶è‡ªåŠ¨åŒ¹é…æœ€æ–°çš„ WeldingDB_*.xlsxï¼‰
    excel_source = r"C:\Projects\PrecomControl\nordinfo"
    resolved_files = resolve_welding_files(excel_source)
    if not resolved_files:
        raise FileNotFoundError(f"æœªåœ¨ {excel_source} æ‰¾åˆ° WeldingDB_*.xlsx æ–‡ä»¶")
    print(f"ä½¿ç”¨ç„Šæ¥æ•°æ®æ–‡ä»¶: {', '.join(resolved_files)}")
    
    # 1. å¯¼å…¥å‰å¤‡ä»½
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ 1/3: åˆ›å»ºå¯¼å…¥å‰å¤‡ä»½...")
    print(f"{'='*60}")
    
    from utils.backup_manager import create_backup
    try:
        backup_id = create_backup(
            trigger='PRE_IMPORT',
            description=f'WeldingListå¯¼å…¥å‰è‡ªåŠ¨å¤‡ä»½ - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'
        )
        print(f"âœ“ å¤‡ä»½å®Œæˆï¼Œå¤‡ä»½ID: {backup_id}")
    except Exception as e:
        print(f"âœ— å¤‡ä»½å¤±è´¥: {e}")
        print("è­¦å‘Š: æœªåˆ›å»ºå¤‡ä»½ï¼Œä½†å°†ç»§ç»­å¯¼å…¥")
        backup_id = None
    
    # 2. ç¡®ä¿è¡¨å­˜åœ¨å¹¶å¯¼å…¥æ•°æ®
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ 2/3: å¯¼å…¥WeldingListæ•°æ®...")
    print(f"{'='*60}")
    
    create_welding_table()
    
    importer = WeldingDataImporter(excel_source, verbose=False)
    import_success = importer.import_to_database()
    
    # 3. å¯¼å…¥åæ™ºèƒ½åŒæ­¥
    if import_success:
        print(f"\n{'='*60}")
        print(f"æ­¥éª¤ 3/3: æ™ºèƒ½åŒæ­¥ä¸»æ•°æ®...")
        print(f"{'='*60}")
        
        from utils.sync_manager import sync_after_import
        try:
            sync_id = sync_after_import(backup_id=backup_id)
            print(f"âœ“ åŒæ­¥å®Œæˆï¼ŒåŒæ­¥ID: {sync_id}")
        except Exception as e:
            print(f"âœ— åŒæ­¥å¤±è´¥: {e}")
            print("è­¦å‘Š: åŒæ­¥å¤±è´¥ï¼Œä½†WeldingListå·²å¯¼å…¥")
        
        print(f"\n{'='*60}")
        print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
        print(f"{'='*60}\n")
    else:
        print(f"\n{'='*60}")
        print(f"âŒ å¯¼å…¥å¤±è´¥ï¼")
        print(f"{'='*60}")
        print(f"æç¤º: å¯ä»¥ä½¿ç”¨å¤‡ä»½ {backup_id} æ¢å¤æ•°æ®")
        print(f"      è¿è¡Œ: python -c \"from utils.restore_manager import restore_backup; restore_backup({backup_id}, preview=False)\"")
        print(f"{'='*60}\n")


